# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "citation.baml": "class Date {\n    day string\n    month string\n    year string\n}\n    \nclass Author {\n    prefix string? @description(\"The title or preffix of the others name eg. Md., Dr., Mr., Ms., etc. Only include if it is not the default.\")\n    first_name string\n    last_name string\n}\n\n\nclass CitationInfo {\n    authors Author[]\n    url string\n    article_title string\n    publication_title string @description(\"The title of the journal or website\")\n    publication_date Date\n    access_date Date\n    volume string? @description(\"The volume of the journal or website\")\n    issue string? @description(\"The issue of the journal or website\")\n    page_range string? @description(\"The page range of the journal or website\")\n    doi string?\n}\n\n\nclass Website {\n    url string\n    content string\n}\n\n\nfunction ExtractCitationInfo(website: Website, access_date: Date) -> CitationInfo {\n    client CustomGemini\n\n    prompt #\"\n        Extract info from this website for citation purposes. Keep the full name of each authors.\n        Additonally do NOT use the references or works cited sections to extract info ONLY the actual article.\n\n        Website Content:\n        ---\n        {{website.content}}\n        ---\n\n        Website URL\n        ---\n        {{website.url}}\n        ---\n\n        Access Date:\n        ---\n        {{access_date}}\n        ---\n\n        {# special macro to print the output schema + instructions #}\n        {{ ctx.output_format }}\n    \"#\n} \n\nfunction GenerateMLACitation(citation_info: CitationInfo) -> string {\n    client CustomGemini\n\n    prompt #\"\n        Generate an MLA website citation with the citation info provided. Respond with only the citation and nothing else.\n        The url needs to be italicized\n\n        ---\n\n        Citation Info:\n        ---\n        {{citation_info}}\n        ---\n\n        {# special macro to print the output schema + instructions #}\n        {{ ctx.output_format }}\n    \"#\n}\n\n\ntest citation_info_test {\n    functions [ExtractCitationInfo]\n    \n    args {\n        website {\n            url \"https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/917\"\n            content \"Title: Why should I use BAML? Author(s): Luke Pitstick, Michael Phelps Date Published: 03/05/2016\"\n        }\n        access_date {\n            day 1\n            month 12\n            year 2025\n        }\n    }\n}\n\ntest generate_mla_citation_test {\n    functions [GenerateMLACitation]\n\n    args {\n        citation_info {\n            authors [\n                {\n                    first_name \"Luke\"\n                    last_name \"Pitstick\"\n                    prefix \"Dr.\"\n                },\n                {\n                    first_name \"Michael\"\n                    last_name \"Phelps\"\n                }\n            ]\n            title \"Why Luke Smells\"\n            url \"lukepitstick.com\"\n            publication_date {\n                day 23\n                month 5\n                year 2025\n            }\n            access_date {\n                day 1\n                month 12\n                year 2025\n            }\n        }\n    }\n\n}",
    "clients.baml": "// Example Ollama client for local models (uncomment to use)\nclient<llm> CustomOllama {\n  provider openai-generic\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2\"\n    default_role \"user\"\n  }\n}\n\nclient<llm> CustomGemma{\n  provider openai-generic\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"gemma3:4b\"\n    default_role \"user\"\n  }\n}\n\nclient<llm> OpenAIClient {\n  provider \"openai-responses\"\n  options {\n    api_key env.OPENAI_API_KEY\n    model \"gpt-5-mini\"\n    temperature 1\n    tools [\n      {\n        type \"web_search\"\n      }\n    ]\n  }\n}\n\nclient<llm> CustomGemini {\n  provider google-ai\n  options {\n    model \"gemini-2.5-flash\"\n    api_key env.GOOGLE_API_KEY\n    default_role \"user\"\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.214.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return _file_map